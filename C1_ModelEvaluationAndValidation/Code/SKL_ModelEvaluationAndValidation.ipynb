{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation And Validation\n",
    "\n",
    "## Tutorial Naive Bayes\n",
    "\n",
    "The Naive Bayes documentation for scikit learn can be found [here](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "\n",
    "### Example: sklearn.naive_bayes.GaussianNB\n",
    "\n",
    "The example code is taken from [here](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "print clf.predict([[-0.8, -1]])\n",
    "\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "\n",
    "print clf_pf.predict([[-0.8, -1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Some metrics for performance in classification models are used here.\n",
    "\n",
    "### Accuracy\n",
    "SciKit learn documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "\n",
    "print accuracy_score(y_true, y_pred)\n",
    "\n",
    "print accuracy_score(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "SciKit learn documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.266666666667\n",
      "0.333333333333\n",
      "0.266666666667\n",
      "[ 0.8  0.   0. ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]\n",
    "\n",
    "print f1_score(y_true, y_pred, average='macro')  \n",
    "\n",
    "print f1_score(y_true, y_pred, average='micro')  \n",
    "\n",
    "print f1_score(y_true, y_pred, average='weighted')  \n",
    "\n",
    "print f1_score(y_true, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "### Simple train test cross validation\n",
    "\n",
    "This example borrows heavily from the example shown on the SciKit learn documentation [here](http://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966666666667\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "###############################################################\n",
    "### YOUR CODE HERE\n",
    "###############################################################\n",
    "\n",
    "### import the relevant code and make your train/test split\n",
    "### name the output datasets features_train, features_test,\n",
    "### labels_train, and labels_test\n",
    "\n",
    "### set the random_state to 0 and the test_size to 0.4 so\n",
    "### we can exactly check your result\n",
    "\n",
    "from sklearn import cross_validation\n",
    "\n",
    "features_train, features_test, labels_train, labels_test \\\n",
    "= cross_validation.train_test_split(features, labels, test_size=0.4, random_state=0)\n",
    "\n",
    "###############################################################\n",
    "\n",
    "clf = SVC(kernel=\"linear\", C=1.)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "print clf.score(features_test, labels_test)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "def submitAcc():\n",
    "    return clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "With simple train test cross validation one of the problems one has is deciding how much data to use for training versus testing. K_Fold cross validation is a way to use all our data for both training and testing.\n",
    "\n",
    "The general idea is that you partition your data into k equal parts and run k separate learning experiments. For each experiment you pick a different testing set and train on the remaining data. You then average test results from those k experiments.\n",
    "\n",
    "Clearly this method is computationally more expensive than the simple train test method but yields a more accurate model.\n",
    "\n",
    "One of the problems which can arise with this method in SKLearn is that the data is partitioned sequentially. Then you have to be careful that there is a good mix of examples in each partition else you might be training on one type of example and then testing on another. There's a simple way to randomize the events in sklearn k-fold CV: set the shuffle flag to true.\n",
    "\n",
    "Instead of something like this:\n",
    "\n",
    "    cv = KFold( len(authors), 2 )\n",
    "\n",
    "one could write:\n",
    "\n",
    "    cv = KFold( len(authors), 2, shuffle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Cross Validation\n",
    "\n",
    "GridCV is a way of systematically working through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. The beauty is that it can work through many combinations in only a couple extra lines of code.\n",
    "\n",
    "Here's an example from the sklearn documentation, which can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html):\n",
    "\n",
    "```\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "```\n",
    "\n",
    "Let's break this down line by line.\n",
    "\n",
    "    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]} \n",
    "\n",
    "A dictionary of the parameters, and the possible values you want to try for them. In this case, they're playing around with the kernel (possible choices are 'linear' and 'rbf'), and C (possible choices are 1 and 10).\n",
    "\n",
    "Then all the following (kernel, C) combinations are automatically generated: [('rbf', 1), ('rbf', 10), ('linear', 1), ('linear', 10)]. Each is used to train an SVM, and the performance is then assessed using cross-validation.\n",
    "\n",
    "    svr = svm.SVC()\n",
    "\n",
    "This looks kind of like creating a classifier, just like we've been doing since the first lesson. But note that the \"clf\" isn't made until the next line--this is just saying what kind of algorithm to use. Another way to think about this is that the \"classifier\" isn't just the algorithm in this case, it's algorithm plus parameter values. Note that there's no monkeying around with the kernel or C; all that is handled in the next line.\n",
    "\n",
    "    clf = grid_search.GridSearchCV(svr, parameters)\n",
    "\n",
    "This is where the first bit of magic happens; the classifier is being created. We pass the algorithm (svr) and the dictionary of parameters to try (parameters) and it generates a grid of parameter combinations to try.\n",
    "\n",
    "    clf.fit(iris.data, iris.target)\n",
    "\n",
    "And the second bit of magic. The fit function now tries all the parameter combinations, and returns a fitted classifier that's automatically tuned to the optimal parameter combination. You can now access the parameter values via clf.best_estimator_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
